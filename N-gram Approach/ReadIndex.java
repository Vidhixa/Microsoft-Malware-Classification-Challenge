import java.io.File;
import java.io.IOException;
import java.util.ArrayList;
import java.util.HashMap;
import java.util.HashSet;
import java.util.LinkedHashMap;
import java.util.List;
import java.util.Map;
import java.util.Map.Entry;
import java.util.Set;

import org.apache.lucene.document.Document;
import org.apache.lucene.index.DirectoryReader;
import org.apache.lucene.index.DocsEnum;
import org.apache.lucene.index.Fields;
import org.apache.lucene.index.IndexReader;
import org.apache.lucene.index.MultiFields;
import org.apache.lucene.index.Term;
import org.apache.lucene.index.Terms;
import org.apache.lucene.index.TermsEnum;
import org.apache.lucene.queryparser.classic.ParseException;
import org.apache.lucene.store.Directory;
import org.apache.lucene.store.FSDirectory;
import org.apache.lucene.util.BytesRef;


public class ReadIndex {

	public void ReadLuceneIndex(int index,String indexDirectory,String fieldName) throws IOException{
		//String indexDirectory = "/media/jus-mine/New Volume/Kaggle/index" + String.valueOf(index) + "/";
		IndexReader reader = DirectoryReader.open(FSDirectory.open(new File(indexDirectory)));
		int NoOfDocs = reader.maxDoc();
		Terms terms;
		TermsEnum termsEnum = null;
		Document doc;
		List<String> tempString;
		List<String> retainedString = getTermsVector(reader, termsEnum, 0,fieldName);
		System.out.println(retainedString.size());
		for( int i = 1; i < reader.maxDoc(); i++ ) {
			tempString = getTermsVector(reader, termsEnum, i,fieldName);
			if(tempString != null ) {
				retainedString.retainAll(tempString);
				System.out.println(retainedString.size());
			}
		}
		System.out.println(retainedString.size());
		for(String str: retainedString)
			System.out.println(str);
	}

	private List<String> getTermsVector(IndexReader reader, TermsEnum termsEnum,int i,String fieldName) throws IOException {
		
		Terms terms; Document doc;
		List<String> tempString;
		doc = reader.document(i);
		terms = reader.getTermVector(i,fieldName);
		//reader.getTermVectors(arg0)
		System.out.println(reader.document(i).get("name") + " " + i);
		
		if(terms  == null){
			
			return null;
		}
		termsEnum = terms.iterator(termsEnum);
		tempString = new ArrayList<String>();
		while(termsEnum.next() != null){
			
			//termsEnum.next();
			tempString.add(termsEnum.term().utf8ToString());
			
		}		
		return tempString;
	}
	
	public Map<String,Long> getAllUniqueTerms(int index,String indexDirectory,String fieldName) throws IOException{				
		//String indexDirectory = "/media/jus-mine/New Volume/Kaggle/index" + String.valueOf(index) + "/";
		IndexReader reader = DirectoryReader.open(FSDirectory.open(new File(indexDirectory)));
		Map<String,Boolean> instructionMap = new HashMap<String, Boolean>();
		ParseUtility.makeMap(instructionMap);
		TermsEnum termsEnum = null; Terms terms;
		Map<String,Long> termFreq = new HashMap<String, Long>();
		String text;
		for(int i = 0; i < reader.maxDoc(); i++){
			terms = reader.getTermVector(i,fieldName);
			if(terms != null) {
				termsEnum = terms.iterator(null);
				while(termsEnum.next() != null) {
					text = termsEnum.term().utf8ToString().trim();
					if(instructionMap.containsKey(text)) {
						if(termFreq.containsKey(text))
							termFreq.put(text,termFreq.get(text) + termsEnum.totalTermFreq());
						else
							termFreq.put(text,termsEnum.totalTermFreq());
					}
				}
			}
		}
		return termFreq;
	}
	
	
	public List<String> listAllFeatures(String indexDirectory,String fieldName,int lowerLimit) throws IOException{
		
		TermsEnum termsEnum = getTermsEnum(indexDirectory, fieldName);
		StringBuilder strBuilder = new StringBuilder(); String text;
		List<String> listOfInstructions = new ArrayList<String>();
		while (termsEnum.next() != null) {
        	text = termsEnum.term().utf8ToString();
        	if(termsEnum.totalTermFreq() > lowerLimit && termsEnum.totalTermFreq() < 70000 && text.indexOf(" ") > -1) {
        		//strBuilder.append(String.format("%s,",text));
        		//String output = strBuilder.substring(0,strBuilder.lastIndexOf(","));
        		
        		listOfInstructions.add(text);
        	}
        }
		return listOfInstructions;
	}
	
	public int readNGrams(String indexDirectory,String fieldName,int lowerLimit) throws IOException{
		TermsEnum termsEnum = getTermsEnum(indexDirectory, fieldName);
        int count = 0; String text;
        
        while (termsEnum.next() != null) {
        	text = termsEnum.term().utf8ToString();
        	if(termsEnum.totalTermFreq() > lowerLimit && termsEnum.totalTermFreq() < 70000 && text.indexOf(" ") > -1) {
        		//System.out.print(text + ",");
        		//System.out.println(text + "," + termsEnum.totalTermFreq());
        		count++;
        	}
        }
        
        return count;
	}

	private TermsEnum getTermsEnum(String indexDirectory, String fieldName)
			throws IOException {
		Directory dir = FSDirectory.open(new File(indexDirectory));
		IndexReader reader = IndexReader.open(dir);
		Fields fields = MultiFields.getFields(reader);
        Terms terms = fields.terms(fieldName);
        TermsEnum termsEnum = terms.iterator(null);
		return termsEnum;
	}
	
	
	public Set<String> featureTest(String fieldName) throws IOException{

		String indexPath;
		int[] testCases = {100,200,300,400,500,600,700,800,900,1000,2000,5000,10000,12000,15000};
		Set<String> set = new HashSet<String>();
		LinkedHashMap<Integer,Integer> featureMap = prepareFeatureDictLimit();
		for(int i = 1; i < 10 ; i++){
		//	System.out.println(String.format("For class %d: ",i));
			indexPath = String.format("/u/ploya/kaggle_train_index/index%d/", i);
			set.addAll(listAllFeatures(indexPath,fieldName,featureMap.get(i)));
			
			/*
			for(int j = 0; j < testCases.length; j++){
				count = readNGrams(indexPath,fieldName,testCases[j]);
				if(count < 10) break;
				System.out.println(testCases[j] + "," +  count);
			}*/
		}
	//	System.out.print(String.join(",", set));
	//	System.out.println(",filename,classLabel");
		StringBuilder strBuilder = new StringBuilder();
		for(String str: set){
			strBuilder.append(String.format("%s,",str));
		}
		//For train
		//System.out.println(String.format("%sfilename,classLabel",strBuilder.toString()));
		//For Test
		System.out.println(String.format("%sfilename,classLabel",strBuilder.toString()));
		return set;	
	}
	
	public LinkedHashMap<Integer,Integer> prepareFeatureDictLimit(){
		LinkedHashMap<Integer,Integer> map = new LinkedHashMap<>();
		map.put(1, 4000);
		map.put(2, 3000);
		map.put(3, 100);
		map.put(4, 300);
		map.put(5, 400);
		map.put(6, 600);
		map.put(7, 415);
		map.put(8, 500);
		map.put(9, 900);
		return map;
	}
	
	public Map<String,Integer> getInitialFeatureFreqDict(Set<String> features){
		Map<String,Integer> featureFreqMap = new HashMap<String, Integer>();
		for(String str: features){
			featureFreqMap.put(str,0);
		}
		return featureFreqMap;
	}
	
	public void FindNGramFreq(String indexDir,int classLabel,Set<String> features) throws IOException, ParseException {
		
		IndexReader reader = DirectoryReader.open(FSDirectory.open(new File(indexDir)));
		Term term; List<Term> terms = new ArrayList<Term>();
		for(String str: features){
			term = new Term("opcodeData",str);
			terms.add(term);
		}
		int noOfDocs = reader.maxDoc();
		Map<String,Map<String,Integer>> map = new HashMap<String, Map<String,Integer>>();
		Map<Integer,String> docNameMap = new HashMap<Integer, String>();
		String temp;
		for(int i = 0; i < reader.maxDoc(); i++){
			temp = reader.document(i).get("name");
			docNameMap.put(i,temp);
			map.put(temp,getInitialFeatureFreqDict(features));
			
		}
		
		DocsEnum docsEnum; Document doc;
		Map<String,Integer> docMap = new HashMap<String, Integer>();
		int count = 0;
		for(Term t : terms) {
			count++;
			docsEnum  = MultiFields.getTermDocsEnum(reader,MultiFields.getLiveDocs(reader),"opcodeData", new BytesRef(t.text()));
			if( docsEnum != null) {
				while (docsEnum.nextDoc() != DocsEnum.NO_MORE_DOCS) {
						temp = docNameMap.get(docsEnum.docID());
						docMap = map.get(temp);		 
						docMap.put(t.text(),docsEnum.freq());
				}	
			}		
		}
		prinClassFeatureMatrix(classLabel,map,features);
	}
	
	public void prinClassFeatureMatrix(int classlabel,Map<String,Map<String,Integer>> fileFeatureList,Set<String> featureList) {
		Map<String,Integer> docMap;// = new HashMap<String, Integer>();
		StringBuilder strBuil;// = new StringBuilder();
		int count = 0;
		for(Entry<String,Map<String,Integer>> entry: fileFeatureList.entrySet()){
			docMap = entry.getValue();
			count++;
			strBuil = new StringBuilder();
			for(String str:featureList){
				//System.out.print(String.format("%d,",docMap.get(str)));
				strBuil.append(String.format("%d,",docMap.get(str)));
			}
			//System.out.println(String.format("%s%s,%d",strBuil.toString(),entry.getKey().replace("/home/vbox/train/", ""),classlabel));
			System.out.println(String.format("%s%s",strBuil.toString(),entry.getKey().replace("/home/vbox/train/", "")));
		}
	}
	
	private void SumOfEachIndex(String indexDir,int classLabel) throws IOException{
		
		IndexReader reader = DirectoryReader.open(FSDirectory.open(new File(indexDir)));
		System.out.println(String.format("%d:%d",classLabel,reader.maxDoc() ));
		reader.close();
	}
	
	public static void main(String[] args) throws IOException, ParseException{
		
		//int classLabel = Integer.parseInt(args[0]);
		//int classLabel = 4;
		ReadIndex rdIndex = new ReadIndex();
		String fieldName = "opcodeData";
		String indexPath;
		//indexPath = String.format("C:\\Study\\Data Mining\\kaggle_train_0427\\index1", classLabel);
		//rdIndex.ReadLuceneIndex(classLabel,indexPath,fieldName);
		//rdIndex.FeatureTest(fieldName);
		Set<String> featureList = rdIndex.featureTest(fieldName);
		for(int i = 0; i < 11; i++) {
			//indexPath = String.format("/u/ploya/kaggle_train_index/index%d/", i);
			indexPath = String.format("/u/ploya/kaggle_test_index/index%d/", i);
			//rdIndex.SumOfEachIndex(indexPath, i);
			
			rdIndex.FindNGramFreq(indexPath,i,featureList);
		}
				
		
	}
	
}
