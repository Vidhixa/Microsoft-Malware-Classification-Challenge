import java.io.IOException;
import java.io.Reader;

import org.apache.lucene.analysis.TokenStream;
import org.apache.lucene.analysis.Tokenizer;
import org.apache.lucene.analysis.ngram.NGramTokenizer;
import org.apache.lucene.analysis.shingle.ShingleFilter;
import org.apache.lucene.analysis.standard.StandardTokenizer;
import org.apache.lucene.analysis.tokenattributes.CharTermAttribute;
import org.apache.lucene.analysis.tokenattributes.OffsetAttribute;
import org.apache.lucene.analysis.util.CharArraySet;
import org.apache.lucene.util.Version;


public class BAnalyzer extends BaseAnalyzer {
	//code omitted
	 private int minGram;
	 private int maxGram;
	 
	  public BAnalyzer(int min,int max) 
	  {
		super(TYPE.CustomNgramAnalyzer,max);
		this.minGram = min;
	    this.maxGram = max;
	  }
		
	@Override
	protected TokenStreamComponents createComponents(String fieldName, Reader reader)		{
		Tokenizer tokenizer = new StandardTokenizer(reader);
		TokenStream filter = new ShingleFilter(tokenizer, 4, 4);
		//OffsetAttribute offsetAttribute = filter.addAttribute(OffsetAttribute.class);
		//CharTermAttribute charTermAttribute = 
		filter.addAttribute(CharTermAttribute.class);
		
		/*try {
			filter.reset();
			while (filter.incrementToken()) {
			//    int startOffset = offsetAttribute.startOffset();
			//    int endOffset = offsetAttribute.endOffset();
			    System.out.println(charTermAttribute.toString());
			}
		} catch (IOException e) {
			// TODO Auto-generated catch block
			e.printStackTrace();
		}*/			
		return new TokenStreamComponents(tokenizer,filter);
	}
	
}